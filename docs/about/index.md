# About GPUX

Docker-like GPU runtime for ML inference with universal GPU compatibility.

---

## What is GPUX?

GPUX is a lightweight, Docker-inspired runtime that makes GPU-accelerated ML inference work everywhere - NVIDIA, AMD, Apple Silicon, Intel, and even CPU-only systems.

### Key Features

- ğŸš€ **Universal GPU Support** - Works on any GPU (NVIDIA, AMD, Apple, Intel)
- ğŸ³ **Docker-like UX** - Familiar `build`, `run`, `serve` commands
- âš¡ **Excellent Performance** - ONNX Runtime with optimized execution providers
- ğŸ”§ **Simple Configuration** - Single `gpux.yml` file
- ğŸ“¦ **Zero Vendor Lock-in** - Use ONNX models from any framework

---

## Why GPUX?

### The Problem

ML deployment is fragmented:
- Different GPUs need different runtimes (CUDA, ROCm, CoreML, DirectML)
- Complex setup and configuration
- Vendor lock-in with frameworks

### The Solution

GPUX provides a unified interface:
```bash
# Works everywhere - same commands, any GPU
gpux build .
gpux run model-name --input '{"data": [1,2,3]}'
gpux serve model-name --port 8080
```

---

## Architecture

GPUX is a **platform layer** built on proven technologies:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GPUX (Docker-like UX)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        ONNX Runtime (Core)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Execution Providers (GPU Backends) â”‚
â”‚  TensorRTâ”‚CUDAâ”‚ROCmâ”‚CoreMLâ”‚DirectML â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Hardware (Any GPU or CPU)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Philosophy**: We focus on UX and tooling, leveraging ONNX Runtime's battle-tested ML execution.

---

## Technology Stack

- **Runtime**: ONNX Runtime (Microsoft)
- **Execution Providers**: TensorRT, CUDA, ROCm, CoreML, DirectML, OpenVINO, CPU
- **CLI**: Typer (Python)
- **Serving**: FastAPI + Uvicorn
- **Configuration**: YAML + Pydantic

---

## Project Status

- âœ… **Production Ready**: Built on mature ONNX Runtime
- ğŸš€ **Active Development**: Regular updates and improvements
- ğŸŒŸ **Open Source**: MIT License

---

## Performance

GPUX delivers excellent performance through optimized execution providers:

| Hardware | Provider | BERT Throughput | vs CPU |
|----------|----------|-----------------|--------|
| RTX 3080 | TensorRT | 2,400 FPS | 48x |
| M2 Pro | CoreML | 450 FPS | 9x |
| RX 6800 XT | ROCm | 600 FPS | 15x |

---

## Get Involved

### Use GPUX
- ğŸ“– [Documentation](../index.md)
- ğŸš€ [Quick Start](../tutorial/installation.md)
- ğŸ’¡ [Examples](../examples/index.md)

### Contribute
- ğŸ› [Report Issues](https://github.com/gpux/gpux-runtime/issues)
- ğŸ’¬ [Discussions](https://github.com/gpux/gpux-runtime/discussions)
- ğŸ¤ [Contributing Guide](contributing.md)

### Stay Updated
- â­ [Star on GitHub](https://github.com/gpux/gpux-runtime)
- ğŸ“° [Changelog](changelog.md)
- ğŸ—ºï¸ [Roadmap](roadmap.md)

---

## License

GPUX is open source under the [MIT License](license.md).
